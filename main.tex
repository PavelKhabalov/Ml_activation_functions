\documentclass[a4paper,12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath,amsfonts,amssymb}
\usepackage{geometry}
\geometry{top=2cm, bottom=2cm, left=2cm, right=2cm}

\begin{document}

\section*{Функции активации в нейронных сетях}

\subsection*{Введение}
Функции активации — это ключевой компонент нейронных сетей, который позволяет моделям улавливать нелинейные зависимости в данных. Без использования этих функций нейронная сеть, независимо от её глубины, сводилась бы к простой линейной модели, неспособной решать сложные задачи, такие как обработка изображений, текста или звука. Функция активации добавляет нелинейность в модель, преобразуя входные данные нейрона перед передачей их в следующий слой.

\subsection*{Концепция функций активации}
Функция активации применяется к выходу линейного преобразования $z = W \cdot x + b$, где $W$ — матрица весов, $x$ — входной вектор, а $b$ — смещение. Полученное значение $z$ затем преобразуется функцией активации, которая определяет, будет ли нейрон активирован и как он повлияет на итоговый результат.

Математически это описывается как:
\[
    a = f(z),
\]
где $f$ — функция активации, а $a$ — выходное значение нейрона. Различные функции активации обладают разными характеристиками и применяются в зависимости от задачи и особенностей данных.

\subsection*{Основные нелинейные функции активации}

\paragraph{1. Sigmoid (Сигмоида)}
\textbf{Формула:}
\[
    f(z) = \frac{1}{1 + e^{-z}}
\]
\textbf{Характеристики:}
\begin{itemize}
    \item Значения находятся в диапазоне от $0$ до $1$.
    \item Хорошо подходит для вероятностных интерпретаций, например, в задачах бинарной классификации.
    \item \textbf{Проблема:} при больших или малых значениях $z$ градиент становится близким к нулю, что замедляет обучение (затухание градиента).
\end{itemize}
\textbf{Применение:} выходной слой для задач бинарной классификации.

\paragraph{2. Tanh (Гиперболический тангенс)}
\textbf{Формула:}
\[
    f(z) = \frac{e^z - e^{-z}}{e^z + e^{-z}}
\]
\textbf{Характеристики:}
\begin{itemize}
    \item Значения находятся в диапазоне от $-1$ до $1$.
    \item Более симметричная функция по сравнению с Sigmoid, что помогает в нормализации данных и ускоряет обучение.
    \item Также подвержена проблеме затухания градиента.
\end{itemize}
\textbf{Применение:} скрытые слои в нейронных сетях, когда требуется нормализация данных.

\paragraph{3. ReLU (Rectified Linear Unit)}
\textbf{Формула:}
\[
    f(z) = \max(0, z)
\]
\textbf{Характеристики:}
\begin{itemize}
    \item Простая и вычислительно эффективная функция.
    \item Устраняет проблему затухания градиента, так как градиент остаётся постоянным для положительных значений.
    \item \textbf{Проблема:} нейроны могут \textit{\"умирать\"}, если выходное значение постоянно остаётся равным нулю (для $z < 0$).
\end{itemize}
\textbf{Применение:} стандартная функция для скрытых слоев глубоких нейронных сетей.

\paragraph{4. Leaky ReLU и Parametric ReLU}
\textbf{Формула Leaky ReLU:}
\[
    f(z) = 
    \begin{cases} 
    z, & z > 0 \\
    \alpha z, & z \leq 0
    \end{cases}
\]
где $\alpha$ — небольшой положительный коэффициент.
\textbf{Характеристики:}
\begin{itemize}
    \item Решает проблему \"мертвых\" нейронов в ReLU, позволяя небольшой градиент при $z < 0$.
\end{itemize}
\textbf{Применение:} альтернативы ReLU в глубоких сетях, где наблюдается проблема \"мертвых\" нейронов.

\paragraph{5. Softmax}
\textbf{Формула:}
\[
    f(z_i) = \frac{e^{z_i}}{\sum_{j} e^{z_j}}
\]
\textbf{Характеристики:}
\begin{itemize}
    \item Преобразует выходные значения в вероятности, которые суммируются до $1$.
    \item Используется в задачах многоклассовой классификации.
\end{itemize}
\textbf{Применение:} выходной слой нейронных сетей для классификации с несколькими классами.

\subsection*{Сравнение функций активации}
\begin{table}[h!]
\centering
\begin{tabular}{|l|l|l|l|l|}
\hline
\textbf{Функция} & \textbf{Диапазон} & \textbf{Градиент} & \textbf{Преимущества} & \textbf{Недостатки} \\
\hline
Sigmoid & (0, 1) & Затухает при больших $|z|$ & Вероятностная интерпретация & Затухание градиента \\
\hline
Tanh & (-1, 1) & Затухает при больших $|z|$ & Симметричность & Затухание градиента \\
\hline
ReLU & [0, $\infty$) & Постоянный для $z > 0$ & Простота, эффективность & \"Мёртвые\" нейроны \\
\hline
Leaky ReLU & (-$\infty$, $\infty$) & Постоянный & Устранение проблемы ReLU & Зависимость от гиперпараметров \\
\hline
Softmax & (0, 1) & Зависит от всех входов & Интерпретация как вероятности & Вычислительная сложность \\
\hline
\end{tabular}
\caption{Сравнение функций активации}
\end{table}

\subsection*{Заключение}
Функции активации являются основой нелинейных моделей в нейронных сетях. Правильный выбор функции активации значительно влияет на качество и скорость обучения сети. Современные архитектуры часто используют комбинации ReLU для скрытых слоев и специализированные функции, такие как Softmax, для выходных слоев, адаптируя их под конкретные задачи.

\subsection*{Задания}

\paragraph{Задание 1.} Какая функция активации преобразует входные значения в диапазон от $-1$ до $1$ и используется для симметричной нормализации данных?

\textbf{Ответ:} Гиперболический тангенс (Tanh).

\paragraph{Задание 2.} Почему функция ReLU является предпочтительной для глубоких нейронных сетей по сравнению с Sigmoid?

\textbf{Ответ:} ReLU устраняет проблему затухания градиента, так как градиент остаётся постоянным для положительных значений. Это ускоряет обучение и делает его более стабильным.

\paragraph{Задание 3.} Какую функцию активации следует использовать в выходном слое модели для задач многоклассовой классификации? Опишите её ключевое свойство.

\textbf{Ответ:} Функция Softmax. Её ключевое свойство — преобразование выходных значений в вероятности, которые суммируются до $1$, что позволяет интерпретировать их как вероятности принадлежности к классам.


\end{document}
